# ğŸ¥¦ vegclassify â€” DÃ©ploiement CPU

Application de classification de lÃ©gumes pour PC sans GPU.  
**Python backend** (FastAPI + PyTorch CPU) Â· **React frontend** Â· Charge les `.pth` directement.

## PrÃ©requis

| Outil | Version |
|-------|---------|
| Python | â‰¥ 3.10 |
| Node.js | â‰¥ 18 (pour build le frontend) |

## Installation

### 1. Cloner et installer les dÃ©pendances Python

```bash
cd vegclassify
pip install -r backend/requirements.txt
```

> Sur certains systÃ¨mes : `pip install -r backend/requirements.txt --break-system-packages`
>
> PyTorch CPU s'installe automatiquement. Si tu veux la version CPU-only plus lÃ©gÃ¨re :
> ```bash
> pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
> pip install fastapi uvicorn[standard] python-multipart Pillow
> ```

### 2. Placer les modÃ¨les

```
vegclassify/
â””â”€â”€ models/
    â”œâ”€â”€ baseline_model.pth    â† Phase A (du pipeline Kaggle)
    â””â”€â”€ hardened_model.pth    â† Phase C (du pipeline Kaggle)
```

### 3. Build le frontend (une seule fois)

```bash
cd frontend
npm ci
npm run build
cd ..
```

Le build produit les fichiers dans `backend/static/`.

### 4. Lancer

```bash
# Linux / Mac
./run.sh

# Windows
run.bat

# Ou directement
python backend/server.py \
    --baseline models/baseline_model.pth \
    --hardened models/hardened_model.pth \
    --port 8080
```

â†’ Ouvrir **http://localhost:8080**

## Usage

L'interface permet de :
- **Charger une image** de lÃ©gume (drag & drop ou clic)
- **Choisir le modÃ¨le** : Base (Phase A), Durci (Phase C), ou Comparer les deux
- **Voir les rÃ©sultats** : top-5 prÃ©dictions avec confiance, temps d'infÃ©rence

En mode **Comparer**, la mÃªme image passe dans les deux modÃ¨les cÃ´te Ã  cÃ´te.

## Structure

```
vegclassify/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py              FastAPI + PyTorch CPU inference
â”‚   â””â”€â”€ requirements.txt       DÃ©pendances Python
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/        8 composants React
â”‚       â”œâ”€â”€ hooks/             State management
â”‚       â”œâ”€â”€ types/             TypeScript interfaces
â”‚       â””â”€â”€ utils/             API client + constantes
â”‚
â”œâ”€â”€ models/                    .pth du pipeline Kaggle
â”œâ”€â”€ run.sh                     Lanceur Linux/Mac
â”œâ”€â”€ run.bat                    Lanceur Windows
â””â”€â”€ README.md
```

## API

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/api/health` | GET | Status, device, modÃ¨les chargÃ©s |
| `/api/models` | GET | Liste des clÃ©s de modÃ¨les |
| `/api/classes` | GET | 15 classes de lÃ©gumes |
| `/api/predict` | POST | InfÃ©rence un modÃ¨le (`file` + `model`) |
| `/api/compare` | POST | InfÃ©rence les deux modÃ¨les (`file`) |

## Options CLI

```
python backend/server.py [options]

  --baseline PATH    ModÃ¨le de base .pth (Phase A)
  --hardened PATH    ModÃ¨le durci .pth (Phase C)
  --port PORT        Port du serveur (dÃ©faut: 8080)
  --host HOST        Adresse d'Ã©coute (dÃ©faut: 127.0.0.1)
```

## DÃ©veloppement

```bash
# Terminal 1 : backend (recharge manuelle)
python backend/server.py --baseline models/baseline_model.pth --hardened models/hardened_model.pth

# Terminal 2 : frontend hot reload (port 3000, proxy â†’ 8080)
cd frontend && npm run dev
```

## Performance CPU

L'infÃ©rence EfficientNet-B0 en PyTorch CPU prend typiquement **50-150ms** par image selon le processeur. C'est suffisant pour un usage interactif.
